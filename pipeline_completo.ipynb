{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Pipeline Completo: Generaci√≥n de Preguntas AWS CLF-C02\n",
    "\n",
    "Este notebook ejecuta todo el pipeline de generaci√≥n de preguntas:\n",
    "1. Fetch de documentaci√≥n AWS oficial\n",
    "2. Construcci√≥n del sistema RAG con ChromaDB\n",
    "3. Estimaci√≥n de costos\n",
    "4. Generaci√≥n de preguntas con GPT-4o-mini\n",
    "5. Evaluaci√≥n con Arize Phoenix\n",
    "\n",
    "---\n",
    "\n",
    "**Requisitos:**\n",
    "- Archivo `.env` con tu `OPENAI_API_KEY`\n",
    "- Dependencias instaladas: `pip install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Setup Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "from IPython.display import display, HTML, Markdown\n",
    "import pandas as pd\n",
    "\n",
    "# Setup paths\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# Load environment\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API key\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    display(HTML('<div style=\"background-color: #ffcccc; padding: 10px; border-radius: 5px;\">‚ùå <b>Error:</b> OPENAI_API_KEY no encontrada en .env<br>Por favor, crea un archivo .env con tu API key</div>'))\n",
    "else:\n",
    "    display(HTML('<div style=\"background-color: #ccffcc; padding: 10px; border-radius: 5px;\">‚úÖ <b>API Key detectada:</b> ' + os.getenv(\"OPENAI_API_KEY\")[:20] + '...</div>'))\n",
    "\n",
    "print(f\"\\nüìÇ Directorio de trabajo: {PROJECT_ROOT}\")\n",
    "print(f\"üïê Inicio del pipeline: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• Paso 1: Fetch Documentaci√≥n AWS\n",
    "\n",
    "Descarga documentaci√≥n oficial de AWS para usar en el sistema RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "DOCS_DIR = PROJECT_ROOT / \"data\" / \"aws_docs\"\n",
    "DOCS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# URLs de documentaci√≥n\n",
    "AWS_DOCS = {\n",
    "    \"CLF-C02 Exam Guide\": \"https://d1.awsstatic.com/training-and-certification/docs-cloud-practitioner/AWS-Certified-Cloud-Practitioner_Exam-Guide.pdf\",\n",
    "    \"Well-Architected Framework\": \"https://docs.aws.amazon.com/wellarchitected/latest/framework/wellarchitected-framework.pdf\",\n",
    "}\n",
    "\n",
    "SERVICE_FAQS = {\n",
    "    \"EC2\": \"https://aws.amazon.com/ec2/faqs/\",\n",
    "    \"S3\": \"https://aws.amazon.com/s3/faqs/\",\n",
    "    \"Lambda\": \"https://aws.amazon.com/lambda/faqs/\",\n",
    "    \"RDS\": \"https://aws.amazon.com/rds/faqs/\",\n",
    "    \"DynamoDB\": \"https://aws.amazon.com/dynamodb/faqs/\",\n",
    "    \"IAM\": \"https://aws.amazon.com/iam/faqs/\",\n",
    "    \"VPC\": \"https://aws.amazon.com/vpc/faqs/\",\n",
    "}\n",
    "\n",
    "def download_file(url, filename, desc=\"\"):\n",
    "    \"\"\"Descarga un archivo\"\"\"\n",
    "    output_path = DOCS_DIR / filename\n",
    "    \n",
    "    if output_path.exists():\n",
    "        print(f\"‚è≠Ô∏è  {desc}: Ya existe\")\n",
    "        return True\n",
    "    \n",
    "    try:\n",
    "        print(f\"üì• Descargando {desc}...\", end=\" \")\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(url, headers=headers, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        with open(output_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        \n",
    "        print(f\"‚úÖ ({len(response.content)/1024:.1f} KB)\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Descargar documentos oficiales\n",
    "print(\"\\nüìö DESCARGANDO DOCUMENTACI√ìN OFICIAL AWS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "success_count = 0\n",
    "for name, url in AWS_DOCS.items():\n",
    "    filename = name.replace(\" \", \"-\") + \".pdf\"\n",
    "    if download_file(url, filename, name):\n",
    "        success_count += 1\n",
    "    time.sleep(1)\n",
    "\n",
    "# Descargar FAQs\n",
    "print(\"\\nüìö DESCARGANDO FAQs DE SERVICIOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for service, url in SERVICE_FAQS.items():\n",
    "    filename = f\"{service.lower()}_faq.html\"\n",
    "    if download_file(url, filename, f\"{service} FAQ\"):\n",
    "        success_count += 1\n",
    "    time.sleep(1)\n",
    "\n",
    "# Resumen\n",
    "total_files = len(list(DOCS_DIR.glob(\"*\")))\n",
    "total_size = sum(f.stat().st_size for f in DOCS_DIR.glob(\"*\")) / (1024 * 1024)\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    'M√©trica': ['Total archivos', 'PDFs', 'HTMLs', 'Tama√±o total'],\n",
    "    'Valor': [\n",
    "        total_files,\n",
    "        len(list(DOCS_DIR.glob(\"*.pdf\"))),\n",
    "        len(list(DOCS_DIR.glob(\"*.html\"))),\n",
    "        f\"{total_size:.2f} MB\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "display(HTML(\"<h3>üìä Resumen de Descarga</h3>\"))\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÆ Paso 2: Construir Sistema RAG\n",
    "\n",
    "Procesa los documentos y crea una base de datos vectorial con ChromaDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader, UnstructuredHTMLLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import tiktoken\n",
    "\n",
    "CHROMA_DIR = PROJECT_ROOT / \"data\" / \"chroma_db\"\n",
    "CHROMA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 50\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "\n",
    "print(\"\\nüìñ CARGANDO DOCUMENTOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "documents = []\n",
    "errors = []\n",
    "\n",
    "# Cargar PDFs\n",
    "pdf_files = list(DOCS_DIR.glob(\"*.pdf\"))\n",
    "print(f\"\\nüìÑ Procesando {len(pdf_files)} PDFs...\")\n",
    "\n",
    "for pdf_path in pdf_files:\n",
    "    try:\n",
    "        loader = PyPDFLoader(str(pdf_path))\n",
    "        docs = loader.load()\n",
    "        for doc in docs:\n",
    "            doc.metadata[\"source\"] = pdf_path.name\n",
    "        documents.extend(docs)\n",
    "        print(f\"   ‚úÖ {pdf_path.name}: {len(docs)} p√°ginas\")\n",
    "    except Exception as e:\n",
    "        errors.append(f\"{pdf_path.name}: {str(e)}\")\n",
    "        print(f\"   ‚ö†Ô∏è  {pdf_path.name}: Error\")\n",
    "\n",
    "# Cargar HTMLs\n",
    "html_files = list(DOCS_DIR.glob(\"*.html\"))\n",
    "print(f\"\\nüåê Procesando {len(html_files)} HTMLs...\")\n",
    "\n",
    "for html_path in html_files:\n",
    "    try:\n",
    "        loader = UnstructuredHTMLLoader(str(html_path))\n",
    "        docs = loader.load()\n",
    "        for doc in docs:\n",
    "            doc.metadata[\"source\"] = html_path.name\n",
    "        documents.extend(docs)\n",
    "        print(f\"   ‚úÖ {html_path.name}\")\n",
    "    except Exception as e:\n",
    "        errors.append(f\"{html_path.name}: {str(e)}\")\n",
    "        print(f\"   ‚ö†Ô∏è  {html_path.name}: Error\")\n",
    "\n",
    "print(f\"\\n‚úÖ Total documentos cargados: {len(documents)}\")\n",
    "\n",
    "# Dividir en chunks\n",
    "print(\"\\n‚úÇÔ∏è  DIVIDIENDO EN CHUNKS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE * 4,\n",
    "    chunk_overlap=CHUNK_OVERLAP * 4,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "chunks = [c for c in chunks if len(c.page_content.strip()) > 50]\n",
    "\n",
    "# Estad√≠sticas\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "sample_tokens = [len(encoding.encode(c.page_content)) for c in chunks[:100]]\n",
    "avg_tokens = sum(sample_tokens) / len(sample_tokens)\n",
    "\n",
    "print(f\"   üìä Chunks creados: {len(chunks)}\")\n",
    "print(f\"   üìä Tokens promedio: ~{avg_tokens:.0f}\")\n",
    "print(f\"   üí∞ Costo estimado embeddings: ${(len(chunks) * avg_tokens / 1_000_000 * 0.02):.4f} USD\")\n",
    "\n",
    "# Crear vector store\n",
    "print(\"\\nüîÆ CREANDO VECTOR DATABASE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"   Modelo embeddings: {EMBEDDING_MODEL}\")\n",
    "print(f\"   Procesando en batches de 100...\")\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL)\n",
    "\n",
    "# Primer batch\n",
    "batch_size = 100\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks[:batch_size],\n",
    "    embedding=embeddings,\n",
    "    persist_directory=str(CHROMA_DIR),\n",
    "    collection_name=\"aws_docs\"\n",
    ")\n",
    "\n",
    "# Resto de batches\n",
    "for i in range(batch_size, len(chunks), batch_size):\n",
    "    batch = chunks[i:i + batch_size]\n",
    "    vectorstore.add_documents(batch)\n",
    "    print(f\"   Procesado: {min(i + batch_size, len(chunks))}/{len(chunks)} chunks\")\n",
    "\n",
    "print(f\"\\n   ‚úÖ Vector store creado en: {CHROMA_DIR}\")\n",
    "\n",
    "# Test de retrieval\n",
    "print(\"\\nüß™ TEST DE RETRIEVAL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_query = \"¬øQu√© es AWS Shield y c√≥mo protege contra DDoS?\"\n",
    "print(f\"\\nüîç Query: {test_query}\")\n",
    "results = vectorstore.similarity_search(test_query, k=3)\n",
    "\n",
    "for i, doc in enumerate(results, 1):\n",
    "    preview = doc.page_content[:150].replace(\"\\n\", \" \")\n",
    "    print(f\"\\n   {i}. Fuente: {doc.metadata.get('source', 'Unknown')}\")\n",
    "    print(f\"      {preview}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí∞ Paso 3: Estimaci√≥n de Costos\n",
    "\n",
    "Calcula el costo de generar preguntas seg√∫n tus necesidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n\n",
    "NUM_QUESTIONS = 50  # üëà CAMBIA ESTO: 50, 100, 200, 300, etc.\n",
    "\n",
    "# Precios OpenAI\n",
    "PRICES = {\n",
    "    \"input\": 0.15 / 1_000_000,\n",
    "    \"output\": 0.60 / 1_000_000\n",
    "}\n",
    "\n",
    "# Dominios CLF-C02\n",
    "DOMAINS = {\n",
    "    \"Domain 1: Cloud Concepts\": 24,\n",
    "    \"Domain 2: Security and Compliance\": 30,\n",
    "    \"Domain 3: Cloud Technology and Services\": 34,\n",
    "    \"Domain 4: Billing, Pricing, and Support\": 12\n",
    "}\n",
    "\n",
    "# Estimaciones\n",
    "tokens_per_question = {\n",
    "    \"input\": 1400,   # Contexto RAG + prompt\n",
    "    \"output\": 400    # Pregunta + opciones + explicaci√≥n\n",
    "}\n",
    "\n",
    "total_input = NUM_QUESTIONS * tokens_per_question[\"input\"]\n",
    "total_output = NUM_QUESTIONS * tokens_per_question[\"output\"]\n",
    "\n",
    "cost_input = total_input * PRICES[\"input\"]\n",
    "cost_output = total_output * PRICES[\"output\"]\n",
    "cost_generation = cost_input + cost_output\n",
    "\n",
    "# Evaluaciones (3 evals por pregunta)\n",
    "eval_tokens_per_question = {\"input\": 300, \"output\": 100}\n",
    "total_evals = NUM_QUESTIONS * 3\n",
    "cost_eval = (total_evals * eval_tokens_per_question[\"input\"] * PRICES[\"input\"] + \n",
    "             total_evals * eval_tokens_per_question[\"output\"] * PRICES[\"output\"])\n",
    "\n",
    "subtotal = cost_generation + cost_eval\n",
    "buffer = subtotal * 0.3\n",
    "total = subtotal + buffer\n",
    "\n",
    "# Distribuci√≥n por dominio\n",
    "distribution = {}\n",
    "for domain, pct in DOMAINS.items():\n",
    "    count = round((pct / 100) * NUM_QUESTIONS)\n",
    "    distribution[domain] = count\n",
    "\n",
    "# Ajustar\n",
    "diff = NUM_QUESTIONS - sum(distribution.values())\n",
    "if diff != 0:\n",
    "    max_domain = max(DOMAINS, key=DOMAINS.get)\n",
    "    distribution[max_domain] += diff\n",
    "\n",
    "# Display\n",
    "display(HTML(f\"<h2>üí∞ Estimaci√≥n de Costos para {NUM_QUESTIONS} Preguntas</h2>\"))\n",
    "\n",
    "# Tabla de distribuci√≥n\n",
    "dist_df = pd.DataFrame([\n",
    "    {\"Dominio\": domain.split(\": \")[1], \"% Oficial\": f\"{DOMAINS[domain]}%\", \"Preguntas\": count}\n",
    "    for domain, count in distribution.items()\n",
    "])\n",
    "display(HTML(\"<h3>üìã Distribuci√≥n por Dominio CLF-C02</h3>\"))\n",
    "display(dist_df)\n",
    "\n",
    "# Tabla de costos\n",
    "cost_df = pd.DataFrame({\n",
    "    'Concepto': [\n",
    "        'Generaci√≥n (input)',\n",
    "        'Generaci√≥n (output)',\n",
    "        'Evaluaciones Phoenix',\n",
    "        'Subtotal',\n",
    "        'Buffer (30%)',\n",
    "        'TOTAL'\n",
    "    ],\n",
    "    'Tokens': [\n",
    "        f\"{total_input:,}\",\n",
    "        f\"{total_output:,}\",\n",
    "        f\"{total_evals * (eval_tokens_per_question['input'] + eval_tokens_per_question['output']):,}\",\n",
    "        '-',\n",
    "        '-',\n",
    "        '-'\n",
    "    ],\n",
    "    'Costo USD': [\n",
    "        f\"${cost_input:.4f}\",\n",
    "        f\"${cost_output:.4f}\",\n",
    "        f\"${cost_eval:.4f}\",\n",
    "        f\"${subtotal:.4f}\",\n",
    "        f\"${buffer:.4f}\",\n",
    "        f\"${total:.4f}\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "display(HTML(\"<h3>üíµ Desglose de Costos</h3>\"))\n",
    "display(cost_df)\n",
    "\n",
    "display(HTML(f'<div style=\"background-color: #e6f3ff; padding: 15px; border-radius: 5px; margin-top: 20px;\"><b>üí° Costo total estimado:</b> ${total:.2f} USD para {NUM_QUESTIONS} preguntas evaluadas</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Paso 4: Generar Preguntas con RAG\n",
    "\n",
    "Usa GPT-4o-mini para generar preguntas basadas en documentaci√≥n real de AWS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "# Cargar prompts\n",
    "with open(PROJECT_ROOT / \"prompts\" / \"system.txt\", 'r', encoding='utf-8') as f:\n",
    "    system_prompt_template = f.read()\n",
    "\n",
    "with open(PROJECT_ROOT / \"prompts\" / \"examples.json\", 'r', encoding='utf-8') as f:\n",
    "    examples = json.load(f)\n",
    "\n",
    "# Topics por dominio\n",
    "DOMAIN_TOPICS = {\n",
    "    \"Domain 1: Cloud Concepts\": [\n",
    "        \"Beneficios de la nube AWS\",\n",
    "        \"Alta disponibilidad y tolerancia a fallos\",\n",
    "        \"Econom√≠a de la nube\",\n",
    "        \"Estrategias de migraci√≥n\"\n",
    "    ],\n",
    "    \"Domain 2: Security and Compliance\": [\n",
    "        \"Modelo de responsabilidad compartida\",\n",
    "        \"AWS IAM\",\n",
    "        \"Servicios de seguridad\",\n",
    "        \"Encriptaci√≥n\"\n",
    "    ],\n",
    "    \"Domain 3: Cloud Technology and Services\": [\n",
    "        \"Amazon EC2\",\n",
    "        \"Amazon S3\",\n",
    "        \"Bases de datos AWS\",\n",
    "        \"Servicios de red\",\n",
    "        \"Lambda y serverless\"\n",
    "    ],\n",
    "    \"Domain 4: Billing, Pricing, and Support\": [\n",
    "        \"Modelos de pricing\",\n",
    "        \"AWS Cost Explorer\",\n",
    "        \"Planes de soporte\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Cliente OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "def get_example_for_domain(domain):\n",
    "    \"\"\"Obtiene ejemplo para el dominio\"\"\"\n",
    "    for ex in examples:\n",
    "        if ex[\"domain\"] == domain:\n",
    "            return ex[\"example\"]\n",
    "    return examples[0][\"example\"]\n",
    "\n",
    "def generate_question(domain, topic, vectorstore):\n",
    "    \"\"\"Genera una pregunta\"\"\"\n",
    "    # Retrieval\n",
    "    query = f\"{domain}: {topic}\"\n",
    "    docs = vectorstore.similarity_search(query, k=3)\n",
    "    context = \"\\n\\n\".join([f\"[{doc.metadata.get('source', 'Unknown')}]\\n{doc.page_content[:500]}\" for doc in docs])\n",
    "    \n",
    "    # Prompt\n",
    "    system_msg = system_prompt_template.format(context=context, domain=domain)\n",
    "    example = get_example_for_domain(domain)\n",
    "    \n",
    "    user_msg = f\"\"\"Genera UNA pregunta de examen sobre: {topic}\n",
    "\n",
    "Sigue el formato del siguiente ejemplo:\n",
    "{json.dumps(example, indent=2, ensure_ascii=False)}\n",
    "\n",
    "Responde SOLO con JSON v√°lido.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_msg},\n",
    "                {\"role\": \"user\", \"content\": user_msg}\n",
    "            ],\n",
    "            temperature=0.8,\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        \n",
    "        question_data = json.loads(response.choices[0].message.content)\n",
    "        question_data[\"domain\"] = domain\n",
    "        question_data[\"topic\"] = topic\n",
    "        question_data[\"retrieved_context\"] = context[:500]\n",
    "        question_data[\"tokens_used\"] = {\n",
    "            \"input\": response.usage.prompt_tokens,\n",
    "            \"output\": response.usage.completion_tokens\n",
    "        }\n",
    "        \n",
    "        return question_data\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Distribuir preguntas\n",
    "distribution_list = []\n",
    "for domain, count in distribution.items():\n",
    "    topics = DOMAIN_TOPICS[domain]\n",
    "    questions_per_topic = count // len(topics)\n",
    "    remainder = count % len(topics)\n",
    "    \n",
    "    for i, topic in enumerate(topics):\n",
    "        topic_count = questions_per_topic + (1 if i < remainder else 0)\n",
    "        for _ in range(topic_count):\n",
    "            distribution_list.append((domain, topic))\n",
    "\n",
    "# Generar preguntas\n",
    "display(HTML(f\"<h3>ü§ñ Generando {len(distribution_list)} preguntas con GPT-4o-mini...</h3>\"))\n",
    "\n",
    "questions = []\n",
    "total_cost = 0.0\n",
    "failed = 0\n",
    "\n",
    "progress_bar = tqdm(distribution_list, desc=\"Generando\")\n",
    "for i, (domain, topic) in enumerate(progress_bar, 1):\n",
    "    q = generate_question(domain, topic, vectorstore)\n",
    "    \n",
    "    if \"error\" not in q:\n",
    "        questions.append(q)\n",
    "        cost = (q[\"tokens_used\"][\"input\"] / 1_000_000 * 0.15 + \n",
    "                q[\"tokens_used\"][\"output\"] / 1_000_000 * 0.60)\n",
    "        total_cost += cost\n",
    "        progress_bar.set_postfix({\"Costo\": f\"${total_cost:.4f}\"})\n",
    "    else:\n",
    "        failed += 1\n",
    "    \n",
    "    # Rate limiting\n",
    "    if i % 10 == 0:\n",
    "        time.sleep(1)\n",
    "\n",
    "# Guardar\n",
    "output_path = PROJECT_ROOT / \"data\" / \"questions_raw.json\"\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(questions, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Resumen\n",
    "result_df = pd.DataFrame({\n",
    "    'M√©trica': ['Generadas', 'Fallidas', 'Costo Real'],\n",
    "    'Valor': [len(questions), failed, f\"${total_cost:.4f} USD\"]\n",
    "})\n",
    "\n",
    "display(HTML(\"<h3>‚úÖ Generaci√≥n Completada</h3>\"))\n",
    "display(result_df)\n",
    "display(HTML(f'<div style=\"background-color: #ccffcc; padding: 10px; border-radius: 5px; margin-top: 10px;\">üìÅ Guardado en: {output_path}</div>'))\n",
    "\n",
    "# Preview\n",
    "if questions:\n",
    "    display(HTML(\"<h3>üëÄ Preview de Primera Pregunta</h3>\"))\n",
    "    q = questions[0]\n",
    "    display(Markdown(f\"**Dominio:** {q['domain']}\"))  \n",
    "    display(Markdown(f\"**Topic:** {q['topic']}\"))  \n",
    "    display(Markdown(f\"**Pregunta:** {q['question']}\"))  \n",
    "    display(Markdown(f\"**Opciones:**\"))\n",
    "    for letter, text in q['options'].items():\n",
    "        marker = \"‚úÖ\" if letter == q['correct_answer'] else \"  \"\n",
    "        display(Markdown(f\"{marker} **{letter})** {text}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Paso 5: Evaluaci√≥n con Phoenix\n",
    "\n",
    "Eval√∫a la calidad de las preguntas generadas usando Arize Phoenix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phoenix as px\n",
    "from phoenix.evals import HallucinationEvaluator, QAEvaluator, OpenAIModel\n",
    "\n",
    "# Iniciar Phoenix\n",
    "display(HTML(\"<h3>üöÄ Iniciando Arize Phoenix...</h3>\"))\n",
    "session = px.launch_app()\n",
    "display(HTML('<div style=\"background-color: #e6f3ff; padding: 10px; border-radius: 5px;\">üìä Dashboard disponible en: <a href=\"http://localhost:6006\" target=\"_blank\">http://localhost:6006</a></div>'))\n",
    "\n",
    "# Setup evaluators\n",
    "eval_model = OpenAIModel(model=MODEL)\n",
    "hallucination_eval = HallucinationEvaluator(eval_model)\n",
    "qa_eval = QAEvaluator(eval_model)\n",
    "\n",
    "def evaluate_clf_compliance(q):\n",
    "    \"\"\"Eval√∫a compliance CLF-C02\"\"\"\n",
    "    checks = {\n",
    "        \"has_4_options\": len(q.get('options', {})) == 4,\n",
    "        \"has_correct_answer\": q.get('correct_answer') in ['A', 'B', 'C', 'D'],\n",
    "        \"has_domain\": q.get('domain', '').startswith('Domain'),\n",
    "        \"has_explanation\": len(q.get('explanation', '')) > 100,\n",
    "        \"answer_in_options\": q.get('correct_answer') in q.get('options', {}),\n",
    "        \"question_not_empty\": len(q.get('question', '')) > 20\n",
    "    }\n",
    "    score = sum(checks.values()) / len(checks)\n",
    "    return score, checks\n",
    "\n",
    "# Evaluar preguntas\n",
    "display(HTML(f\"<h3>üîç Evaluando {len(questions)} preguntas...</h3>\"))\n",
    "\n",
    "approved = []\n",
    "rejected = []\n",
    "eval_results = []\n",
    "\n",
    "progress = tqdm(questions, desc=\"Evaluando\")\n",
    "for i, q in enumerate(progress, 1):\n",
    "    # Eval 1: Hallucination\n",
    "    input_text = f\"{q['question']}\\n\\n\" + \"\\n\".join([f\"{k}) {v}\" for k, v in q['options'].items()])\n",
    "    try:\n",
    "        hall_result = hallucination_eval.evaluate(\n",
    "            input=input_text,\n",
    "            output=q['explanation'],\n",
    "            context=q.get('retrieved_context', '')\n",
    "        )\n",
    "        hall_score = hall_result.score\n",
    "    except:\n",
    "        hall_score = 0.5\n",
    "    \n",
    "    # Eval 2: QA Correctness\n",
    "    try:\n",
    "        qa_result = qa_eval.evaluate(\n",
    "            input=q['question'],\n",
    "            output=f\"{q['correct_answer']}) {q['options'][q['correct_answer']]}\",\n",
    "            reference=q['explanation']\n",
    "        )\n",
    "        qa_score = qa_result.score\n",
    "    except:\n",
    "        qa_score = 0.5\n",
    "    \n",
    "    # Eval 3: Compliance\n",
    "    clf_score, checks = evaluate_clf_compliance(q)\n",
    "    \n",
    "    # Decidir\n",
    "    passed = (hall_score < 0.3 and qa_score > 0.7 and clf_score >= 0.9)\n",
    "    \n",
    "    result = {\n",
    "        \"hallucination\": hall_score,\n",
    "        \"qa_correctness\": qa_score,\n",
    "        \"clf_compliance\": clf_score,\n",
    "        \"passed\": passed\n",
    "    }\n",
    "    eval_results.append(result)\n",
    "    \n",
    "    q_with_eval = {**q, \"phoenix_evals\": result}\n",
    "    \n",
    "    if passed:\n",
    "        approved.append(q_with_eval)\n",
    "    else:\n",
    "        reasons = []\n",
    "        if hall_score >= 0.3:\n",
    "            reasons.append(f\"Hallucination: {hall_score:.2f}\")\n",
    "        if qa_score <= 0.7:\n",
    "            reasons.append(f\"QA: {qa_score:.2f}\")\n",
    "        if clf_score < 0.9:\n",
    "            reasons.append(f\"Compliance: {clf_score:.2f}\")\n",
    "        q_with_eval[\"rejection_reasons\"] = reasons\n",
    "        rejected.append(q_with_eval)\n",
    "    \n",
    "    progress.set_postfix({\"Aprobadas\": len(approved), \"Rechazadas\": len(rejected)})\n",
    "\n",
    "# Guardar resultados\n",
    "approved_path = PROJECT_ROOT / \"data\" / \"questions_evaluated.json\"\n",
    "rejected_path = PROJECT_ROOT / \"data\" / \"questions_rejected.json\"\n",
    "\n",
    "with open(approved_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(approved, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(rejected_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(rejected, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Reporte\n",
    "total = len(questions)\n",
    "approval_rate = (len(approved) / total * 100) if total > 0 else 0\n",
    "\n",
    "avg_hall = sum(r[\"hallucination\"] for r in eval_results) / len(eval_results)\n",
    "avg_qa = sum(r[\"qa_correctness\"] for r in eval_results) / len(eval_results)\n",
    "avg_clf = sum(r[\"clf_compliance\"] for r in eval_results) / len(eval_results)\n",
    "\n",
    "report_df = pd.DataFrame({\n",
    "    'M√©trica': [\n",
    "        'Total Evaluadas',\n",
    "        '‚úÖ Aprobadas',\n",
    "        '‚ùå Rechazadas',\n",
    "        'Tasa de Aprobaci√≥n',\n",
    "        '',\n",
    "        'Hallucination (avg)',\n",
    "        'QA Correctness (avg)',\n",
    "        'CLF Compliance (avg)'\n",
    "    ],\n",
    "    'Valor': [\n",
    "        total,\n",
    "        len(approved),\n",
    "        len(rejected),\n",
    "        f\"{approval_rate:.1f}%\",\n",
    "        '',\n",
    "        f\"{avg_hall:.3f}\",\n",
    "        f\"{avg_qa:.3f}\",\n",
    "        f\"{avg_clf:.3f}\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "display(HTML(\"<h2>üìä REPORTE FINAL DE EVALUACIONES</h2>\"))\n",
    "display(report_df)\n",
    "\n",
    "display(HTML(f'<div style=\"background-color: #ccffcc; padding: 15px; border-radius: 5px; margin-top: 20px;\">'\n",
    "             f'<b>‚úÖ Preguntas aprobadas:</b> {len(approved)}<br>'\n",
    "             f'<b>üìÅ Guardadas en:</b> {approved_path}<br><br>'\n",
    "             f'<b>‚ùå Preguntas rechazadas:</b> {len(rejected)}<br>'\n",
    "             f'<b>üìÅ Guardadas en:</b> {rejected_path}<br><br>'\n",
    "             f'<b>üìä Phoenix Dashboard:</b> <a href=\"http://localhost:6006\" target=\"_blank\">http://localhost:6006</a>'\n",
    "             f'</div>'))\n",
    "\n",
    "# Preview de pregunta aprobada\n",
    "if approved:\n",
    "    display(HTML(\"<h3>‚úÖ Ejemplo de Pregunta Aprobada</h3>\"))\n",
    "    q = approved[0]\n",
    "    display(Markdown(f\"**Dominio:** {q['domain']}\"))  \n",
    "    display(Markdown(f\"**Pregunta:** {q['question']}\"))  \n",
    "    for letter, text in q['options'].items():\n",
    "        marker = \"‚úÖ\" if letter == q['correct_answer'] else \"  \"\n",
    "        display(Markdown(f\"{marker} **{letter})** {text}\"))\n",
    "    display(Markdown(f\"**Scores Phoenix:**\"))\n",
    "    display(Markdown(f\"- Hallucination: {q['phoenix_evals']['hallucination']:.3f} (< 0.3 ‚úÖ)\"))\n",
    "    display(Markdown(f\"- QA Correctness: {q['phoenix_evals']['qa_correctness']:.3f} (> 0.7 ‚úÖ)\"))\n",
    "    display(Markdown(f\"- CLF Compliance: {q['phoenix_evals']['clf_compliance']:.3f} (>= 0.9 ‚úÖ)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Pipeline Completado\n",
    "\n",
    "¬°Listo! Has generado y evaluado preguntas de calidad para el examen AWS CLF-C02.\n",
    "\n",
    "### Pr√≥ximos pasos:\n",
    "\n",
    "1. **Revisar preguntas aprobadas** en `data/questions_evaluated.json`\n",
    "2. **Explorar Phoenix Dashboard** en http://localhost:6006 para ver an√°lisis detallado\n",
    "3. **Integrar al simulador** copiando las preguntas aprobadas a `data/questions.json`\n",
    "\n",
    "### Para integrar con el simulador web:\n",
    "\n",
    "```python\n",
    "# Copiar preguntas al simulador\n",
    "import shutil\n",
    "shutil.copy(\n",
    "    PROJECT_ROOT / \"data\" / \"questions_evaluated.json\",\n",
    "    PROJECT_ROOT / \"data\" / \"questions.json\"\n",
    ")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
