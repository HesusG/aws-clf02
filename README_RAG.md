# ğŸ¤– Sistema RAG + GPT-4o-mini + Phoenix Evals

Generador inteligente de preguntas de examen AWS CLF-C02 usando:
- **RAG (Retrieval-Augmented Generation)** con documentaciÃ³n oficial AWS
- **GPT-4o-mini** para generaciÃ³n de preguntas de alta calidad
- **Arize Phoenix** para evaluaciÃ³n automÃ¡tica de calidad

---

## ğŸ¯ Â¿QuÃ© hace este sistema?

Genera preguntas de examen AWS CLF-C02 que son:
- âœ… **Basadas en docs oficiales AWS** (no inventadas)
- âœ… **Evaluadas automÃ¡ticamente** para evitar hallucinations
- âœ… **Con explicaciones pedagÃ³gicas** detalladas
- âœ… **Distribuidas segÃºn dominios oficiales** CLF-C02
- âœ… **Ultra econÃ³micas**: ~$0.30 USD por 300 preguntas

---

## ğŸ“Š Costos Estimados

| Preguntas | GeneraciÃ³n | EvaluaciÃ³n | Total |
|-----------|------------|------------|-------|
| 100 | $0.05 | $0.03 | **~$0.10** |
| 200 | $0.10 | $0.06 | **~$0.20** |
| 300 | $0.15 | $0.09 | **~$0.30** |
| 500 | $0.25 | $0.15 | **~$0.50** |

*Precios con buffer de 30% incluido*

---

## ğŸš€ Setup RÃ¡pido

### 1. Instalar dependencias

```bash
pip install -r requirements.txt
```

### 2. Configurar API Key

```bash
# Copiar ejemplo
cp .env.example .env

# Editar .env y agregar tu OPENAI_API_KEY
nano .env
```

### 3. Ejecutar Pipeline Completo

```bash
# Paso 1: Descargar docs AWS (~2 min)
python scripts/1_fetch_aws_docs.py

# Paso 2: Construir RAG (~5 min, costo: ~$0.01)
python scripts/2_build_rag.py

# Paso 3: Calcular costos (interactivo)
python scripts/3_estimate_cost.py

# Paso 4: Generar preguntas (~10 min para 300 preguntas)
python scripts/4_generate_questions.py --count 300

# Paso 5: Evaluar con Phoenix (~15 min)
python scripts/5_evaluate_with_phoenix.py
```

---

## ğŸ“ Estructura del Proyecto

```
aws-clf02/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ 1_fetch_aws_docs.py       # Descarga docs AWS
â”‚   â”œâ”€â”€ 2_build_rag.py             # Crea vector DB
â”‚   â”œâ”€â”€ 3_estimate_cost.py         # Calculadora de costos
â”‚   â”œâ”€â”€ 4_generate_questions.py    # Generador con RAG
â”‚   â””â”€â”€ 5_evaluate_with_phoenix.py # Phoenix evals
â”‚
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ system.txt                 # System prompt
â”‚   â””â”€â”€ examples.json              # Few-shot examples
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ aws_docs/                  # Docs descargadas
â”‚   â”œâ”€â”€ chroma_db/                 # Vector database
â”‚   â”œâ”€â”€ questions_raw.json         # Pre-evaluaciÃ³n
â”‚   â”œâ”€â”€ questions_evaluated.json   # âœ… Aprobadas
â”‚   â””â”€â”€ questions_rejected.json    # âŒ Rechazadas
â”‚
â””â”€â”€ .env                           # Tu OPENAI_API_KEY
```

---

## ğŸ”§ Scripts Detallados

### Script 1: Fetch AWS Docs

Descarga documentaciÃ³n oficial:
- AWS CLF-C02 Exam Guide (PDF)
- Well-Architected Framework (PDF)
- Cloud Adoption Framework (HTML)
- 17 Service FAQs (EC2, S3, Lambda, RDS, etc.)

```bash
python scripts/1_fetch_aws_docs.py
```

**Output:**
- `data/aws_docs/` con ~20 documentos
- ~10 MB de documentaciÃ³n oficial

### Script 2: Build RAG

Procesa docs y crea vector database:
- Divide en chunks de ~500 tokens
- Genera embeddings con `text-embedding-3-small`
- Almacena en ChromaDB (local, gratuito)

```bash
python scripts/2_build_rag.py
```

**Output:**
- `data/chroma_db/` con vector database
- Costo: ~$0.01 USD (one-time)

### Script 3: Cost Estimator

Calculadora interactiva:
- Ingresa nÃºmero de preguntas deseadas
- Ve distribuciÃ³n por dominio CLF-C02
- ObtÃ©n estimaciÃ³n detallada de costos
- OpciÃ³n de ejecutar generador directamente

```bash
python scripts/3_estimate_cost.py
```

**Ejemplo de salida:**
```
Â¿CuÃ¡ntas preguntas deseas generar? 300

DISTRIBUCIÃ“N POR DOMINIO CLF-C02
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Cloud Concepts
   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 24%
   72 preguntas

Security and Compliance
   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 30%
   90 preguntas

...

TOTAL ESTIMADO: $0.31 USD
```

### Script 4: Generate Questions

Genera preguntas usando RAG + GPT-4o-mini:
- Busca contexto relevante (top-3 chunks)
- Inyecta en prompt con few-shot examples
- Genera pregunta + 4 opciones + explicaciÃ³n
- Progress bar en tiempo real

```bash
python scripts/4_generate_questions.py --count 300 --output questions_raw.json
```

**Opciones:**
- `--count`: NÃºmero de preguntas (default: 50)
- `--output`: Archivo de salida (default: questions_raw.json)

**Output:**
```json
{
  "question": "Una empresa...",
  "options": {
    "A": "...",
    "B": "...",
    "C": "...",
    "D": "..."
  },
  "correct_answer": "B",
  "explanation": "**Respuesta correcta: B) ...**\n\n...",
  "domain": "Domain 3: Cloud Technology and Services",
  "topic": "Amazon S3 caracterÃ­sticas",
  "retrieved_context": "AWS docs...",
  "tokens_used": {"input": 1450, "output": 380}
}
```

### Script 5: Phoenix Evaluations

EvalÃºa calidad con 3 evaluators:

1. **Hallucination Detection** (score < 0.3 para aprobar)
   - Detecta si inventa informaciÃ³n
   - Verifica contra docs AWS reales

2. **QA Correctness** (score > 0.7 para aprobar)
   - Valida que pregunta tenga sentido
   - Verifica que respuesta correcta sea correcta

3. **CLF-C02 Compliance** (score >= 0.9 para aprobar)
   - 4 opciones vÃ¡lidas
   - Formato correcto
   - Dominio oficial

```bash
python scripts/5_evaluate_with_phoenix.py --input questions_raw.json
```

**Output:**
- `data/questions_evaluated.json` - âœ… Aprobadas
- `data/questions_rejected.json` - âŒ Rechazadas
- Phoenix Dashboard: http://localhost:6006

**Ejemplo de reporte:**
```
REPORTE DE EVALUACIONES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Total evaluadas: 300
âœ… Aprobadas: 267 (89.0%)
âŒ Rechazadas: 33 (11.0%)

Scores Promedio:
   Hallucination: 0.154 (menor es mejor, lÃ­mite: 0.3)
   QA Correctness: 0.863 (mayor es mejor, lÃ­mite: 0.7)
   CLF Compliance: 0.967 (mayor es mejor, lÃ­mite: 0.9)
```

---

## ğŸ¨ Phoenix Dashboard

Al ejecutar el Script 5, Phoenix abre un dashboard en http://localhost:6006

**Features del dashboard:**
- ğŸ“Š MÃ©tricas de calidad en tiempo real
- ğŸ” Explorar preguntas rechazadas
- ğŸ“ˆ DistribuciÃ³n de scores
- ğŸ”¬ Traces de cada llamada a GPT
- ğŸ’° Uso de tokens y costos

---

## ğŸ’¡ Uso Avanzado

### Generar solo para un dominio especÃ­fico

Edita `scripts/4_generate_questions.py` y modifica `DOMAINS`:

```python
# Solo Security & Compliance
DOMAINS = {
    "Domain 2: Security and Compliance": 100
}
```

### Ajustar criterios de evaluaciÃ³n

Edita `scripts/5_evaluate_with_phoenix.py`:

```python
# MÃ¡s estricto
passed = (
    hall_result["score"] < 0.2 and  # Era 0.3
    qa_result["score"] > 0.8 and    # Era 0.7
    compliance_result["score"] >= 0.95  # Era 0.9
)
```

### Cambiar modelo

En `.env`:
```bash
OPENAI_MODEL=gpt-4o  # Mejor calidad pero mÃ¡s caro
# o
OPENAI_MODEL=gpt-4o-mini  # MÃ¡s econÃ³mico (default)
```

---

## ğŸ“ Integrar con Simulador

Para usar las preguntas generadas en el simulador web:

```bash
# Copiar preguntas evaluadas al simulador
cp data/questions_evaluated.json data/questions.json

# O combinar con preguntas existentes
python -c "
import json
with open('data/questions.json') as f: old = json.load(f)
with open('data/questions_evaluated.json') as f: new = json.load(f)
old['questions'].extend(new)
with open('data/questions.json', 'w') as f: json.dump(old, f, indent=2)
"
```

---

## ğŸ› Troubleshooting

### Error: OPENAI_API_KEY no encontrada
```bash
# Verificar que .env existe
ls -la .env

# Verificar contenido
cat .env

# Debe contener:
# OPENAI_API_KEY=sk-...
```

### Error: ChromaDB no encontrado
```bash
# Ejecutar primero el script 2
python scripts/2_build_rag.py
```

### Rate limit de OpenAI
El script 4 ya incluye rate limiting (1 seg cada 10 preguntas).
Si aÃºn hay errores, aumenta el delay en `scripts/4_generate_questions.py`:

```python
# LÃ­nea ~260
if i % 10 == 0:
    time.sleep(2)  # Era 1 segundo
```

### Phoenix no abre dashboard
```bash
# Verificar puerto 6006 disponible
lsof -i :6006

# Cambiar puerto en .env
PHOENIX_PORT=6007
```

---

## ğŸ“š Referencias

- [OpenAI API Pricing](https://openai.com/api/pricing/)
- [Arize Phoenix Docs](https://arize.com/docs/phoenix/)
- [AWS CLF-C02 Exam Guide](https://d1.awsstatic.com/training-and-certification/docs-cloud-practitioner/AWS-Certified-Cloud-Practitioner_Exam-Guide.pdf)
- [LangChain RAG](https://python.langchain.com/docs/use_cases/question_answering/)

---

## âœ… Checklist de Uso

- [ ] InstalÃ© dependencias (`pip install -r requirements.txt`)
- [ ] ConfigurÃ© `OPENAI_API_KEY` en `.env`
- [ ] EjecutÃ© `1_fetch_aws_docs.py`
- [ ] EjecutÃ© `2_build_rag.py`
- [ ] CalculÃ© costos con `3_estimate_cost.py`
- [ ] GenerÃ© preguntas con `4_generate_questions.py`
- [ ] EvaluÃ© con Phoenix `5_evaluate_with_phoenix.py`
- [ ] RevisÃ© dashboard de Phoenix
- [ ] IntegrÃ© preguntas aprobadas al simulador

---

**Â¡Listo! Ahora tienes un sistema completo de generaciÃ³n de preguntas AWS CLF-C02 con calidad garantizada.** ğŸš€
